python -m src.experiment \
--pan \
--model att_bert \
--pretrained_model sentence-transformers/quora-distilbert-multilingual \
--tokenizer sentence-transformers/quora-distilbert-multilingual \
--input_mode hierarchical \
--task pan_hatespeech \
--num_labels 2 \
--max_seq_len 32 \
--attention \
--model_file trained_models/quora-distilbert-multilingual-tokens_en_es_pan_epochs_5_max_32_lr_3e_5_with_attention_cv_10 \
--dropout 0.1 \
--data data/en \
--test_batch_size 100 \
--lang en \
--output_dir ./en_alt/